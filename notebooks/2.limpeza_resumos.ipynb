{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "Preparo dos textos dos resumos das teses e dissertações <br/>\n",
    "<img src=\"https://dadosabertos.capes.gov.br/img/caixa.png\"  alt=\"Dados Capes\"/>\n",
    "</h1>\n",
    "\n",
    "\n",
    "\n",
    "Em nosso conjunto de dados possuímos informações sobre teses e dissertações defendidas no período de 1987-2022. No notebook de [download de metadados](notebooks/1.download_catalogos.ipynb), realizamos o download e junção de todos os conjuntos.\n",
    "\n",
    "Desejamos extrair informações e estatísticas sobre o conteúdo das teses e dissertações, para isso utilizamos a coluna `DS_RESUMO`. Antes de realizar a análise, é necessário realizar um pré-processamento dos textos, removendo elementos que não são relevantes para a análise. Nesse notebook, exploramos os textos dos resumos em busca de padrões e elementos que podem ser removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "workdir = Path(\"..\")\n",
    "df = pd.read_parquet(workdir / \"data\" / \"catalogo-de-teses-e-dissertacoes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teses sem título ou resumo não serão consideradas na análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"NM_PRODUCAO\", \"DS_RESUMO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos as seguintes colunas da análise\n",
    "\n",
    "- `NM_DISCENTE`: Nome do discente\n",
    "- `DS_URL_TEXTO_COMPLETO`: URL para o texto completo\n",
    "- `NR_PAGINAS`: Número de páginas\n",
    "- `NR_VOLUME`: Número do volume\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    columns=[\"NM_DISCENTE\", \"DS_URL_TEXTO_COMPLETO\", \"NR_PAGINAS\", \"NR_VOLUME\"]\n",
    ")\n",
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando a distribuição do tamanho dos textos, podemos observar que 75% dos textos possuem até 2516 caracteres. O maior texto possui 32.767 caracteres. Na próxima seção, vamos trabalhar na identificação de padrões e elementos que podem ser removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"DS_RESUMO\"] = df[\"DS_RESUMO\"].str.lower()\n",
    "df = df.sort_values(\"AN_BASE\").drop_duplicates(\"DS_RESUMO\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DS_RESUMO\"].str.len().describe().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza do Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No conjunto de dados,há textos formados apenas por caracteres especiais ou repetidos. Removemos esse padrão de texto, pois não é relevante para a análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df[\"DS_RESUMO\"].str.match(r\"^(.)\\1+$\")]  # remove caracteres repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos a situação onde o texto é formado majoritariamente por caracteres especiais, como `#`, `*`, `@`, `!`, `?`, `&`, `+`, `=`, `~`, `^`, `%`, `|`, `;`, `:`, `.`. Primeiro calculamos a proporção desses caracteres, mantendo apenas os textos onde a proporção é menor que 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PROPORCAO_ESPECIAIS\"] = (\n",
    "    df[\"DS_RESUMO\"].str.count(r\"[^\\w\\s]\") / df[\"DS_RESUMO\"].str.len()\n",
    ")\n",
    "# manter apenas textos com menos de 20% de caracteres especiais\n",
    "df = df.loc[df[\"PROPORCAO_ESPECIAIS\"] < 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro caso presente no conjunto são termos como `NONONO` ou `AAAAAAAAAA`. Também removemos esses textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DS_RESUMO\"] = df[\"DS_RESUMO\"].str.replace(r\"([\\w.*-][\\w.*-])\\1{2,}\", \"\", regex=True)\n",
    "df = df.drop_duplicates(\"DS_RESUMO\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando teses com até 50 caracteres, observamos que muitos dos textos não são relevantes para a análise, pois são compostos por caracteres repetidos ou informações com pouco significado. Abaixo, temos alguns exemplos de textos com até 50 caracteres. \n",
    "\n",
    "- `este estudo`\n",
    "- `considerando o momento político...`\n",
    "- `kklçklçk`\n",
    "- `kjgutdrs`\n",
    "- `não contém a informação`\n",
    "- `fekgh`\n",
    "- `tgsc`\n",
    "- `sdfskdjfksj`\n",
    "- `será inserido após correçoes.`\n",
    "- `fgfggh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df.loc[df[\"DS_RESUMO\"].str.len() <= 50, \"DS_RESUMO\"].sample(10).values:\n",
    "    print(f\"- `{t}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"DS_RESUMO\"].str.len() > 50]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df[\"DS_RESUMO\"].str.cat(sep=\" \").split()\n",
    "types = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total de palavras: {len(words):,}\")\n",
    "print(f\"Tamanho do vocabulário: {len(types):,}\")\n",
    "print(f\"Riqueza do corpus: {len(types) / len(words):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hapax legomena**\n",
    "\n",
    "Hapax legomena é uma palavra ou expressão que ocorre apenas uma vez dentro de um contexto: seja no registro escrito de uma língua inteira, nas obras de um autor ou em um único texto. Em nosso conjunto de dados, temos 5 milhões de palavras únicas. Como forma de acelerar o processamento e reduzir a complexidade, removemos palavras que ocorrem apenas uma vez no conjunto de dados, pois não são relevantes para a tarefa de extração de tópicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapaxes = set([word for word, count in types.items() if count == 1])\n",
    "print(f\"Hapax legomena: {len(hapaxes):,}\")\n",
    "print(f\"Proporção de hapax legomena: {len(hapaxes) / len(types):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DS_RESUMO\"] = df[\"DS_RESUMO\"].progress_apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in hapaxes])\n",
    ")\n",
    "df = df.drop_duplicates(\"DS_RESUMO\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"DS_RESUMO\"].str.len() > 50]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar alguns resumos, vemos que há textos com informações sobre a entrega da versão final da dissertação ou tese. Abaixo, temos alguns exemplos de textos com informações sobre a entrega da versão final. \n",
    "\n",
    "- o aluno ainda disponibilizou a versão final da dissertação.\n",
    "- o resumo será apresentada na versão final da dissertação.\n",
    "- o aluno não disponibilizou a dissertação de mestrado.\n",
    "- tese não enviada à biblioteca ou ainda não catalogada.\n",
    "- a aluna defendeu, mas ainda não entregou o material final.\n",
    "- aluna ainda não entregou a versão final da dissertação.\n",
    "- defesa fechada não autorizada divulgação pela aluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"DS_RESUMO\"].str.len() < 60, \"DS_RESUMO\"].sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lidar com esses textos, baixamos um modelo pré-treinado em português do Word2Vec, o objetivo é identificar sentenças relacionadas à entrega da versão final da dissertação ou tese. Com isso, podemos remover essas sentenças dos textos.\n",
    "\n",
    "```bash\n",
    "wget http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s300.zip -O cbow_s300.zip\n",
    "unzip cbow_s300.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [model[word] for word in words if word.isalpha() and word in model]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(workdir / \"cbow_s300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df[\"DS_RESUMO\"].progress_apply(get_sentence_vector)\n",
    "vectors = np.stack(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo recuperamos as sentenças mais similares à entrega da versão final da dissertação ou tese.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(\n",
    "    vectors,\n",
    "    get_sentence_vector(\"o aluno ainda disponibilizou a versão final da dissertação.\"),\n",
    ")\n",
    "idxs = np.argsort(similarities.ravel())[::-1]\n",
    "print(similarities[idxs[:50]])\n",
    "df.iloc[idxs[:50]][\"DS_RESUMO\"].values\n",
    "df.loc[(similarities >= 0.0006), \"DS_RESUMO\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após observar os textos mais similares, definimos um limiar para identificar sentenças relacionadas à entrega da versão final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"o aluno ainda disponibilizou a versão final da dissertação.\",\n",
    "    \"o resumo será apresentada na versão final da dissertação.\",\n",
    "    \"o aluno não disponibilizou a dissertação de mestrado.\",\n",
    "    \"tese não enviada à biblioteca ou ainda não catalogada.\",\n",
    "    \"a aluna defendeu, mas ainda não entregou o material final.\",\n",
    "    \"aluna ainda não entregou a versão final da dissertação.\",\n",
    "    \"o resumo será enviado juntamente com o texto definitivo da dissertação\",\n",
    "    \"a dissertação está passando por correções de conteúdo e será entregue futuramente.\",\n",
    "    \"defesa fechada - proibida a divulgação da dissertação por estar em processo de registro de patente\",\n",
    "    \"o resumo da dissertação encontra-se no estágio de correções acadêmicas e gráficas, sugeridas pela banca examinadora.\",\n",
    "    \"até o momento não foi depositada na secretaria o resumo.\",\n",
    "    \"a aluna nao entregou a dissertacao com sua devidas correcoes na secretaria ate esta data.\",\n",
    "    \"o referido aluno nao entregou a versao definitiva ainda\",\n",
    "    \"aguardando a discente entregar as cópias da versão final da tese com o devido aval de sua orientadora.\",\n",
    "    \"a partir da data de defesa, a aluna tem 45 dias para a entrega da versão definitiva da sua dissertação.\",\n",
    "    \"até o presente momento não foi entregue o resumo da tese.\",\n",
    "    \"encontra-se à disposição no site: \",\n",
    "    \"produção intelectual ainda não foi entregue\",\n",
    "    \"não foi entregue pelo discente ao programa a versão final após a defesa\",\n",
    "    \"aguardando trabalho final do discente\",\n",
    "]\n",
    "\n",
    "threshold = 0.00065\n",
    "for text in texts:\n",
    "    similarities = cosine_similarity(vectors, get_sentence_vector(text))\n",
    "    df = df.loc[~(similarities >= threshold)]\n",
    "    vectors = vectors[~(similarities >= threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = (\n",
    "        text.lower()\n",
    "        .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        .translate(str.maketrans(\"\", \"\", string.digits))\n",
    "        .strip()\n",
    "    )\n",
    "    text = \" \".join(\n",
    "        [word for word in text.split() if len(word) > 1 or word in [\"a\", \"o\"]]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"DS_RESUMO_LIMPO\"] = df[\"DS_RESUMO\"].progress_apply(preprocess_text)\n",
    "df = df.loc[df[\"DS_RESUMO_LIMPO\"].str.len() > 20]\n",
    "df = df.drop_duplicates(\"DS_RESUMO_LIMPO\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"PROPORCAO_ESPECIAIS\"]).to_parquet(\n",
    "    workdir / \"data\" / \"catalogo-de-teses-e-dissertacoes-limpo.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
